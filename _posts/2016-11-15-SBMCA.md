---
layout: post
title:  "Semi-Blind Morphological Component Analysis"
date:   2016-11-15 21:36:43
categories: blog
---

This post is an overview of my thesis work carried out at the Department of Electrical and Computer Engineering, University of Minnesota-Twin Cities. My thesis work examines a semi-blind source separation problem which finds applications in audio, image, and video processing. The primary aim of this tutorial is to develop an overall intuitive understanding of my work. Suitable examples from our day-to-day lives are employed to motivate and develop the algorithmic framework. 

# Introduction
If you have ever been to a really noisy party or a large gathering, you must have noticed that you can turn your attention to a specific person and <i>listen</i> to him/her in spite of all the other sounds. It is interesting how you can recognize and keep track of which voice(s) to follow. This process of selective hearing, in signal processing literature, is referred to as the <i>cocktail-party</i> problem or the source separation problem. This problem has been well studied in literature and is of great interest in the signal processing community. However, most of the previous work in this area deals with separating the sources based on either of two simplifying assumptions [[ICA]],

* The sources (speakers) which are inherently <i>independent</i>, meaning they sound very differently to begin with.
* The methods assume that multiple mixtures, equal to the number of individual sources, are available.


In real life situations such <i>independence</i> and availability of these multiple mixtures of the sources are luxuries that we seldom have. For the source separation task we set out to solve at the cocktail-party, the voices of the speakers are somewhat similar, and we say that the sources (voices of individual speakers) are correlated. In addition, because we perform this task (separation of one speaker's sound from many) on a single recording, this is a <i>single channel</i> source separation task. In this regard, one can view the source separation task as disentangling a mixture of often correlated sources. In cases where none of the sources are known <i>a priori</i>, the problem is known as a Blind Source Separation Problem and when knowledge about some of the sources is available, we refer to it as a Semi-Blind Source Separation Problem. 

# A Motivating Example in Audio Forensics

The semi-blind source separation problem studied in this work is motivated from its application in electroshock law enforcement devices. These devices, when fired, eject probes which attach to the suspect and deliver electric current to restrict their movement. A microphone mounted on the gun records the sounds during the event, which is a mixture of ambient noise (people talking/shouting) along with a signal which indicates the status of the device (whether it was delivering current or not). It is often of interest to determine whether the probes were successfully attached and delivering current. However, due to the ambient noise, the fact the two states of the device (delivering and not-delivering current) are highly correlated and other non-idealities, we have a somewhat difficult task at hand. 


# A visit to the Minnesota Orchestra

After the hostile situation we encountered above, it is time we relax a little by visiting the Minnesota Orchestra. Suppose that you are at the Minnesota Orchestra and after the first act your friends are excitedly discussing about the Cello in the piece you just heard. Now, imagine you have no idea about how the Cello sounds like. What would you do?

You have the following options:

* <strong>Case 1</strong>: Listen to a sample of a Cello before the next act. 
* <strong>Case 2:</strong> Assuming that you can identify all other instruments, try identifying the Cello and learn what it sounds like during the next act.


These two processes by which you recognize the instrument (Cello) are at the heart of Machine Learning literature. In the cases described above, you either have the training data (Case 1) to learn the instruments or you have the knowledge about all other instruments and you learn the features specific to Cello, on the fly, from the data (Case 2, referred to as an <i>online </i>methodology). 

In the situation described above, we often employ the terms like <i>training</i> and <i>learning</i>. Before we delve into the algorithmic specifics of the original problem of semi-blind source separation we set out to solve, let us get introduced to these terms and a new technique available in data analysis and machine learning viz Dictionary Learning.

## Dictionary Learning
The word dictionary paints a picture of a resource which contains the meanings of all the words of a language. In an analogous fashion, one can create a dictionary of types of cats, dogs etc., which gives us information about types of cats or dogs respectively. It is assumed that a dictionary of dogs must not be used to look up cats and <i>vice versa</i>. However, it is important to note that both cats and dogs have some features like four feet, two-eyes etc. which are common. In this regard, we note that a good dictionary for a specific class has the distinguishing features of that class i.e. the features that can be used to identify the class. 

There is yet another term to be tackled before we describe Dictionary Learning, this is <i>Sparsity</i>. When we say that a signal has a <i>Sparse</i> representation in a dictionary, we mean that that signal can be formed by using a few elements of a specific Dictionary.  It is like saying that you have a set of spices and you use a certain (small) number of spices for a specific dish. These individual spices will form the <i>sparse</i> basis for all spice preparations you will use in you meals. All these individual spices hence form the dictionary of spices. 

Now, Dictionary Learning is a process by which we aim to learn these distinguishing features of a class from a dataset (known as <i>training data</i>). This technique essentially learns the <i>sparsifying</i> features of a dataset. So why are we interested in sparsifying dictionaries (dictionaries which yield sparse representations)? Sparsifying dictionaries help us to represent the signals succinctly, such compact representations can help us define a particular class efficiently. When such dictionaries are learned on the go, without the use of prior training, it is known as <i>Online Dictionary Learning</i> [[Mairal 2010]]. 


# Semi-Blind Morphological Component Analysis
<img src="/docs/sbmca_asilomar_data/Model_sims.png" style="width:550px;display:block; margin: 0px auto;margin-top: -5px;margin-bottom: 5px;"/>


## Revisiting the Cellist: the SBMCA Algorithmic framework

We describe the algorithmic motivation to solve the single channel semi-blind source separation problem based on the <i>Online</i> Dictionary Learning technique discussed above by using the technique similar to Case 2 described in Section 3. 

Imagine that you have individual dictionaries containing the defining features of each instrument being played at the orchestra except for the Cello. Our essential aim here is to find the part of the musical piece which cannot be represented in these <i>a priori</i> known dictionaries and learn a new dictionary of features describing this unrepresented part (or instrument). This new dictionary (for our example) hence describes the Cello. 

This forms the algorithmic basis of the technique developed to solve the single channel semi-blind source separation problem. More formally, in each iteration of the algorithm we aim to find the representation of a signal in a set of known dictionaries and in the next step learn the unknown dictionary. After a few iterations, we have the desired source separation via their representations in these distinct dictionaries. We call this algorithmic framework as the Semi-Blind Morphological Component Analysis (SBMCA) as it is similar to the Morphological Component Analysis (MCA) [[Starck 2005]], [[Donoho 2010]], [[Bobin 2007]] technique for which all dictionaries are known.

## Back to the Audio Forensics Application
For the Audio Forensics application described in Section 2, we can obtain the information about the delivering and non-delivering current states by triggering the device in a laboratory setting. We form the known dictionaries by using this information about the state signals of the device. In addition, we use the algorithm described above to separate the state signal from the unknown background signal. Subsequently, the state signal can be used to identify the time instants at which the current was being delivered by the electro-shock law enforcement device. 



# Results

<img src="/docs/sbmca_asilomar_data/methods_and_dicts.png" width="710">
<br>
<!--<div><img src="/docs/sbmca_asilomar_data/methods_and_dicts.png" style="width:300px;display:block; margin: 0px auto;margin-top: -5px;margin-bottom: 5px;"/></div>-->



<table cellpadding="2" cellspacing="8" align="center">
  <tr>
  <td colspan="3" > <div text-align="center" style="margin-top:-80px;margin-left:300px;"> <strong>Original Mixture</strong></div><br>
  <img text-align="center" style="width:300px;display:block;margin-left: 235px;margin-right: auto;margin-top:-10px;" src="/docs/sbmca_asilomar_data/OrigMix.png" /> <br>
  <audio align="center" style="width:300px;display:block;margin-left: 235px;margin-right: auto;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/OrigMix.wav"></source><p>Your browser does not support the audio element.</p></audio> </td>
  </tr>
<!--SBMCA-->
  <tr>
    <td rowspan="2" > <div style="-moz-transform: rotate(-90.0deg);-o-transform: rotate(-90.0deg);-webkit-transform: rotate(-90.0deg);filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);transform: rotate(-90.0deg);float:left;" >SBMCA</div></td>
    <td><img src="/docs/sbmca_asilomar_data/XpSBMCA.png" alt="Drawing" style="width:300px;float:left;"/> </td>
    <td><img src="/docs/sbmca_asilomar_data/XuSBMCA.png" alt="Drawing" style="width:300px;float:left;"/></td>
  </tr>
  <tr>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XpSBMCA.wav"></source><p>Your browser does not support the audio element.</p></audio> </td>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XuSBMCA.wav"></source><p>Your browser does not support the audio element.</p></audio></td>
  </tr>
  <!--DCT-->
  <tr>
    <td rowspan="2" > <div style="-moz-transform: rotate(-90.0deg);-o-transform: rotate(-90.0deg);-webkit-transform: rotate(-90.0deg);filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);transform: rotate(-90.0deg);float:left;" >MCA DCT</div></td>
    <td><img src="/docs/sbmca_asilomar_data/XpDCT.png" alt="Drawing" style="width:300px;float:left;"/> </td>
    <td><img src="/docs/sbmca_asilomar_data/XuDCT.png" alt="Drawing" style="width:300px;float:left;"/></td>
  </tr>
  <br>
  <tr>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XpDCT.wav"></source><p>Your browser does not support the audio element.</p></audio> </td>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XuDCT.wav"></source><p>Your browser does not support the audio element.</p></audio></td>
  </tr>
  <!--EYE-->
  <tr>
    <td rowspan="2" > <div style="-moz-transform: rotate(-90.0deg);-o-transform: rotate(-90.0deg);-webkit-transform: rotate(-90.0deg);filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);transform: rotate(-90.0deg);float:left;" >MCA Identity</div></td>
    <td><img src="/docs/sbmca_asilomar_data/XpEYE.png" alt="Drawing" style="width:300px;float:left;"/> </td>
    <td><img src="/docs/sbmca_asilomar_data/XuEYE.png" alt="Drawing" style="width:300px;float:left;"/></td>
  </tr>
  <br>
  <tr>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XpEYE.wav"></source><p>Your browser does not support the audio element.</p></audio> </td>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XuEYE.wav"></source><p>Your browser does not support the audio element.</p></audio></td>
  </tr>
  <!--NNSC-->
  <tr>
    <td rowspan="2" > <div style="-moz-transform: rotate(-90.0deg);-o-transform: rotate(-90.0deg);-webkit-transform: rotate(-90.0deg);filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);transform: rotate(-90.0deg);float:left;" >NNSC</div></td>
    <td><img src="/docs/sbmca_asilomar_data/XpNNSC.png" alt="Drawing" style="width:300px;float:left;"/> </td>
    <td><img src="/docs/sbmca_asilomar_data/XuNNSC.png" alt="Drawing" style="width:300px;float:left;"/></td>
  </tr>
  <br>
  <tr>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XpNNSC.wav"></source><p>Your browser does not support the audio element.</p></audio> </td>
    <td><audio style="width:300px;display:block;float:left;margin-top: -10px;" controls preload><source type="audio/wav" src="/docs/sbmca_asilomar_data/XuNNSC.wav"></source><p>Your browser does not support the audio element.</p></audio></td>
  </tr>
  <br>
   <br>
</table>




# Conclusions
We motivate and propose a technique to solve the single channel semi-blind source separation problem via Semi-Blind Morphological Component Analysis. We base our discussion on intuition and motivating examples from day to day life. For a deeper analysis, algorithmic details and the results please see [[Rambhatla 2013]].

[ICA](http://www.sciencedirect.com/science/article/pii/S0893608000000265)
[UMTC](https://twin-cities.umn.edu)
[Mairal 2010](http://www.jmlr.org/papers/v11/mairal10a.html)
[Starck 2005](http://ieeexplore.ieee.org/abstract/document/1510691/)
[Donoho 2010](https://arxiv.org/abs/1004.3006)
[Bobin 2007](http://ieeexplore.ieee.org/abstract/document/4337756/)
[Rambhatla 2013](http://ieeexplore.ieee.org/document/6810587/)

<!--
<br/>
<script  src="//platform.linkedin.com/in.js" type="text/javascript"> lang: en_US</script>
<script type="IN/Share"></script>    <a href="https://twitter.com/share" class="twitter-share-button" data-text="An interesting read " data-via="siri_r" data-count="none">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>-->

